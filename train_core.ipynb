{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 셋팅하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\begas\\Desktop\\Project\\SmartFarm\\1. DAT\\22_이엔티DB_신천농장_외부.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------|\n",
    "# ----- Step 1. Settings ------------------------\n",
    "# -----------------------------------------------|\n",
    "os.chdir(\"C:/Users/begas/Desktop/Project/SmartFarm\")\n",
    "os.getcwd()\n",
    "\n",
    "home_path  = os.getcwd()\n",
    "data_path  = os.path.join(home_path,\"1. DAT\")\n",
    "save_path  = os.path.join(home_path,\"2. OUT\")\n",
    "model_path = os.path.join(home_path,\"3. MODEL\")\n",
    "\n",
    "data_files = os.path.join(data_path,os.listdir(data_path)[6])\n",
    "print(data_files)\n",
    "\n",
    "working_data = '20220916'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOAD_FN(data_files : os.path) -> pd.DataFrame :\n",
    "    '''\n",
    "    * 입력\n",
    "    data_files : raw 데이터 위치\n",
    "    \n",
    "    * 출력\n",
    "    raw_dat    : 데이터 프레임\n",
    "    '''\n",
    "    # 데이터 불러오기\n",
    "    raw_dat = pd.read_csv(data_files)\n",
    "    \n",
    "    # 데이터 변수명 변경\n",
    "    if 'sensingDt' in raw_dat.columns : raw_dat.rename(columns = {'sensingDt' : '시간'    }, inplace = True)\n",
    "    if 'nh3'       in raw_dat.columns : raw_dat.rename(columns = {'nh3'       : '암모니아'}, inplace = True)\n",
    "    if 'h2s'       in raw_dat.columns : raw_dat.rename(columns = {'h2s'       : '황화수소'}, inplace = True)\n",
    "    if 'tmp'       in raw_dat.columns : raw_dat.rename(columns = {'tmp'       : '온도'    }, inplace = True)\n",
    "    if 'hum'       in raw_dat.columns : raw_dat.rename(columns = {'hum'       : '습도'    }, inplace = True)\n",
    "    if 'voc'       in raw_dat.columns : raw_dat.rename(columns = {'voc'       : '환기팬'  }, inplace = True)\n",
    "        \n",
    "    # Time : date 변수로 변경\n",
    "    raw_dat['시간'] = pd.to_datetime(raw_dat['시간'])\n",
    "    \n",
    "    # 제거1. 암모니아와 황화수소가 두개의 변수일 경우 ppm으로 선택\n",
    "    if len([s for s in raw_dat.columns if '암모니아' in s]) > 1 : \n",
    "        del_cols = [s for s in raw_dat.columns if ('mV' in s) or ('(㎷)' in s)]\n",
    "        raw_dat = raw_dat.drop(columns = del_cols)\n",
    "\n",
    "    if len([s for s in raw_dat.columns if '황화수소' in s]) > 1 : \n",
    "        del_cols = [s for s in raw_dat.columns if ('mV' in s) or ('(㎷)' in s)]\n",
    "        raw_dat = raw_dat.drop(columns = del_cols)\n",
    "        \n",
    "    # 제거2. 변수명에 영어 포함시 영어 제거\n",
    "    cols_list = []\n",
    "    for cols in raw_dat.columns:\n",
    "        result = re.sub(\"[a-zA-Z]|[^\\w\\s]\", \"\", cols)\n",
    "        cols_list.append(result)\n",
    "    raw_dat.columns = cols_list\n",
    "    \n",
    "    # 제거3. 모든 값이 NA인 변수 제거\n",
    "    del_cols = raw_dat.columns[raw_dat.isna().mean() == 1]\n",
    "    if len(del_cols) > 0 :\n",
    "        print('모든 값이 NA인 변수 제거 :','/'.join(del_cols))\n",
    "        raw_dat = raw_dat.drop(columns = del_cols)\n",
    "\n",
    "    # 제거4. 단일값\n",
    "    col_unq_val = raw_dat.apply(lambda xx : len(xx.unique()), axis = 0)\n",
    "    del_cols    = raw_dat.columns[col_unq_val == 1]\n",
    "    if len(del_cols) > 0 :\n",
    "        print('모든 값이 단일값인 변수 제거 :','/'.join(del_cols))\n",
    "        raw_dat = raw_dat.drop(columns = del_cols) \n",
    "    \n",
    "    return raw_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 값이 단일값인 변수 제거 : 온도/환기팬\n"
     ]
    }
   ],
   "source": [
    "load_dat = LOAD_FN(data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터 전처리 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PREPROCESS_FN(DAT : pd.DataFrame, time_grp : int) -> pd.DataFrame :\n",
    "    '''\n",
    "    * 입력\n",
    "    DAT              : LOAD_FN의 출력 데이터 프레임\n",
    "    time_grp         : 시간대별 통계량 요약 기준 단위(ex : if time_grp = 60, then 60분 단위로 통계량 요약)\n",
    "    \n",
    "    * 출력\n",
    "    final_summary_df : 데이터 프레임\n",
    "    '''\n",
    "    \n",
    "    print(\"Step1. 이상치를 허용범위 내로 보정\")\n",
    "    if '환기팬'       in DAT.columns : DAT.loc[( DAT['환기팬']       <   0 ) & (~DAT['환기팬'].isna())      ,'환기팬']       =   0\n",
    "    if '암모니아'     in DAT.columns : DAT.loc[( DAT['암모니아']     <   0 ) & (~DAT['암모니아'].isna())    ,'암모니아']     =   0\n",
    "    if '황화수소'     in DAT.columns : DAT.loc[( DAT['황화수소']     <   0 ) & (~DAT['황화수소'].isna())    ,'황화수소']     =   0\n",
    "    if '거품도포량'   in DAT.columns : DAT.loc[( DAT['거품도포량']   <   0 ) & (~DAT['거품도포량'].isna())  ,'거품도포량']   =   0\n",
    "    if '거품도포시간' in DAT.columns : DAT.loc[( DAT['거품도포시간'] <   0 ) & (~DAT['거품도포시간'].isna()),'거품도포시간'] =   0\n",
    "        \n",
    "    if '온도' in DAT.columns : DAT.loc[( DAT['온도'] >  50 ) & (~DAT['온도'].isna()),'온도'] =  50\n",
    "    if '온도' in DAT.columns : DAT.loc[( DAT['온도'] < -50 ) & (~DAT['온도'].isna()),'온도'] = -50\n",
    "    if '습도' in DAT.columns : DAT.loc[( DAT['습도'] > 100 ) & (~DAT['습도'].isna()),'습도'] = 100\n",
    "    if '습도' in DAT.columns : DAT.loc[( DAT['습도'] <   0 ) & (~DAT['습도'].isna()),'습도'] =   0\n",
    "        \n",
    "    print(\"Step2. 시간 변수를\", time_grp, \"분 단위로 변경\")\n",
    "    \n",
    "    def floor_dt(dt, time_grp = time_grp):\n",
    "        # how many secs have passed this day\n",
    "        nsecs = dt.hour*3600 + dt.minute*60 + dt.second + dt.microsecond*1e-6\n",
    "        delta = nsecs % (time_grp * 60)\n",
    "        return dt - datetime.timedelta(seconds=delta)\n",
    "    \n",
    "    DAT['시간'] = DAT['시간'].apply(floor_dt)  \n",
    "    \n",
    "    print(\"Step3. 시간별 요약통계량 데이터 생성\")\n",
    "    summary_cols = [s for s in DAT.columns if '시간' not in s]\n",
    "    mean_df = DAT.groupby('시간').apply(lambda xx : xx[summary_cols].mean(skipna = True)).reset_index(drop = False).rename(columns = {s:s+\"_mean\" for s in summary_cols})\n",
    "    min_df  = DAT.groupby('시간').apply(lambda xx : xx[summary_cols].min(skipna = True)).reset_index(drop = True).rename(columns = {s:s+\"_min\" for s in summary_cols})\n",
    "    max_df  = DAT.groupby('시간').apply(lambda xx : xx[summary_cols].max(skipna = True)).reset_index(drop = True).rename(columns = {s:s+\"_max\" for s in summary_cols})\n",
    "    std_df  = DAT.groupby('시간').apply(lambda xx : xx[summary_cols].std(skipna = True)).reset_index(drop = True).rename(columns = {s:s+\"_std\" for s in summary_cols})\n",
    "\n",
    "    # 요약 통계량 데이터 프레임 생성\n",
    "    summary_df = pd.concat([mean_df,min_df,max_df,std_df], axis = 1)\n",
    "    \n",
    "    print(\"Step4. 특정 시간대의 데이터가 비어있을 시 해당 시간대 생성\")\n",
    "    st_time = summary_df['시간'].iloc[0]\n",
    "    ed_time = summary_df['시간'].iloc[-1]\n",
    "    base_date_df = pd.DataFrame({'시간' : pd.date_range(st_time,ed_time, freq = 'H')})\n",
    "    n_missing_hour = len(set(set(base_date_df['시간'])) - set(set(summary_df['시간'])))\n",
    "    print(f'> {n_missing_hour}개의 시간대가 비어있습니다. 해당 시간대를 생성합니다.')\n",
    "    \n",
    "    print(\"Step5. 선형 보간법 적용\")\n",
    "    final_summary_df = pd.merge(base_date_df,summary_df, how = 'left', on = '시간')\n",
    "    final_summary_df.iloc[:,1:] = final_summary_df.iloc[:,1:].interpolate(method='linear')\n",
    "    \n",
    "    return final_summary_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1. 이상치를 허용범위 내로 보정\n",
      "Step2. 시간 변수를 60 분 단위로 변경\n",
      "Step3. 시간별 요약통계량 데이터 생성\n",
      "Step4. 특정 시간대의 데이터가 비어있을 시 해당 시간대 생성\n",
      "> 22개의 시간대가 비어있습니다. 해당 시간대를 생성합니다.\n",
      "Step5. 선형 보간법 적용\n"
     ]
    }
   ],
   "source": [
    "preprocess_result_df = PREPROCESS_FN(load_dat, time_grp = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 - 분석용 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRAIN_ANAL_DAT_FN(dat : pd.DataFrame, time_window : list) -> dict :\n",
    "    print(\"Step1. 목표변수: 암모니아, 황화수소 단위별 max 값\")\n",
    "    dat.rename(columns = {'암모니아_max' : 'y_암모니아', '황화수소_max' : 'y_황화수소','시간' : 'base_time'}, inplace = True)\n",
    "    dat.rename(columns = {s : 'x_' + s for s in dat.columns if ('time' not in s) and ('y_' not in s)}, inplace = True)\n",
    "    \n",
    "    print(\"Step2. 시차 변수 생성\")\n",
    "    old_dat = dat.copy()\n",
    "    new_dat = dat.copy()\n",
    "    \n",
    "    new_dat = new_dat[['base_time'] + [s for s in new_dat.columns if 'y_' in s]].copy()\n",
    "    new_dat.rename(columns = {'base_time' : 'predict_time'}, inplace = True)\n",
    "    \n",
    "    xvar_list = [s for s in old_dat.columns if 'x_' in s]\n",
    "    for w in time_window :\n",
    "        new_dat['base_time'] =\\\n",
    "        new_dat['predict_time'].apply(lambda xx : pd.date_range(end = xx, periods = w + 1, freq = 'H')[0])\n",
    "\n",
    "        new_dat = \\\n",
    "        pd.merge(new_dat, old_dat[['base_time'] + xvar_list].rename(columns = {s : s +'_'+str(w) for s in xvar_list})\n",
    "                , how = 'left'\n",
    "                , on  = 'base_time')\n",
    "        \n",
    "    # 시차 데이터로 인한 결측 제거\n",
    "    new_dat = new_dat.iloc[max(time_window):,:].reset_index(drop = True)\n",
    "    \n",
    "    print(\"Step3. 학습/검증 데이터 7:3으로 분할\")\n",
    "    train_dat = new_dat.iloc[:math.ceil(new_dat.shape[0] * 0.7),:].reset_index(drop = True)\n",
    "    test_dat  = new_dat.iloc[math.ceil(new_dat.shape[0] * 0.7):,:].reset_index(drop = True)\n",
    "    \n",
    "    print(\"Step4. 단일 값만 가지는 설명변수 제거\")\n",
    "    # 학습 데이터에서 단일값만 가지는 변수 제거\n",
    "    xvar_list = [s for s in train_dat.columns if 'x_' in s]\n",
    "    single_value_var_index = np.where(train_dat[xvar_list].apply(lambda xx : xx.nunique()) == 1)[0]\n",
    "    single_value_var_names = [xvar_list[s] for s in single_value_var_index]\n",
    "    xvar_list = list(set(xvar_list) - set(single_value_var_names))\n",
    "    xvar_list.sort()\n",
    "    \n",
    "    ret = dict({'train_dat' : train_dat, 'test_dat' : test_dat, 'x_var' : xvar_list})\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1. 목표변수: 암모니아, 황화수소 단위별 max 값\n",
      "Step2. 시차 변수 생성\n",
      "Step3. 학습/검증 데이터 7:3으로 분할\n",
      "Step4. 단일 값만 가지는 설명변수 제거\n"
     ]
    }
   ],
   "source": [
    "TRAIN_ANAL_DAT = TRAIN_ANAL_DAT_FN(preprocess_result_df, time_window = [12,24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.ensemble        import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid, KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MODELING_MLR(yvar_name : str, dat : pd.DataFrame) -> dict :\n",
    "    '''\n",
    "    * 입력\n",
    "    yvar_name : 종속변수 명\n",
    "    dat       : TRAIN_ANAL_DAT의 출력 데이터 프레임\n",
    "    \n",
    "    * 출력\n",
    "    dict      : 모델 및 모델 관련 정보들을 가지고 있는 딕셔너리\n",
    "    '''\n",
    "    print(f'Model Type is MLR')\n",
    "    \n",
    "    # 학습에 사용할 설명변수 명 지정\n",
    "    xvar_name = TRAIN_ANAL_DAT['x_var']\n",
    "\n",
    "    # 학습, 검증 데이터 준비\n",
    "    mlr_train_y = np.array(TRAIN_ANAL_DAT['train_dat'][yvar_name])\n",
    "    mlr_train_x = np.array(TRAIN_ANAL_DAT['train_dat'][xvar_name])\n",
    "    mlr_test_x  = np.array(TRAIN_ANAL_DAT['test_dat'][xvar_name])\n",
    "\n",
    "    # 회귀분석 Fitting\n",
    "    mlr_model = LinearRegression()\n",
    "    mlr_model.fit(X=mlr_train_x, y = mlr_train_y)\n",
    "\n",
    "    # 학습/검증데이터 예측\n",
    "    TRAIN_ANAL_DAT['train_dat']['pred'] = mlr_model.predict(X=mlr_train_x)\n",
    "    TRAIN_ANAL_DAT['test_dat']['pred'] = mlr_model.predict(X=mlr_test_x)\n",
    "    ret = dict({'model' : mlr_model,'model_name' : 'MLR' , \"yvar\" : yvar_name, \"xvar\" : xvar_name, \"train_res\" : TRAIN_ANAL_DAT['train_dat'], \"test_res\" : TRAIN_ANAL_DAT['test_dat']})\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type is MLR\n"
     ]
    }
   ],
   "source": [
    "MLR_RESULT = MODELING_MLR(yvar_name = 'y_암모니아', dat = TRAIN_ANAL_DAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MODELING_RF(yvar_name : str, dat : pd.DataFrame) -> dict :\n",
    "    '''\n",
    "    * 입력\n",
    "    yvar_name : 종속변수 명\n",
    "    dat       : TRAIN_ANAL_DAT의 출력 데이터 프레임\n",
    "    \n",
    "    * 출력\n",
    "    dict      : 모델 및 모델 관련 정보들을 가지고 있는 딕셔너리\n",
    "    '''\n",
    "    print(f'Model Type is Random Forest')\n",
    "    \n",
    "    # 학습에 사용할 설명변수 명 지정\n",
    "    xvar_name = TRAIN_ANAL_DAT['x_var']\n",
    "\n",
    "    # 학습, 검증 데이터 준비\n",
    "    rf_train_y = np.array(TRAIN_ANAL_DAT['train_dat'][yvar_name])\n",
    "    rf_train_x = np.array(TRAIN_ANAL_DAT['train_dat'][xvar_name])\n",
    "    rf_test_x  = np.array(TRAIN_ANAL_DAT['test_dat'][xvar_name])\n",
    "    \n",
    "    # Random Forest Fitting\n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    rf_model.fit(X=rf_train_x, y = rf_train_y)\n",
    "    \n",
    "    # 학습/검증데이터 예측\n",
    "    TRAIN_ANAL_DAT['train_dat']['pred'] = rf_model.predict(rf_train_x)\n",
    "    TRAIN_ANAL_DAT['test_dat']['pred']  = rf_model.predict(rf_test_x)\n",
    "    ret = dict({'model' : rf_model,'model_name' : 'RF' , \"yvar\" : yvar_name, \"xvar\" : xvar_name, \"train_res\" : TRAIN_ANAL_DAT['train_dat'], \"test_res\" : TRAIN_ANAL_DAT['test_dat']})\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type is Random Forest\n"
     ]
    }
   ],
   "source": [
    "RF_RESULT = MODELING_RF(yvar_name = 'y_암모니아', dat = TRAIN_ANAL_DAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MODELING_XGB(yvar_name : str, dat : pd.DataFrame) -> dict :\n",
    "    '''\n",
    "    * 입력\n",
    "    yvar_name : 종속변수 명\n",
    "    dat       : TRAIN_ANAL_DAT의 출력 데이터 프레임\n",
    "    \n",
    "    * 출력\n",
    "    dict      : 모델 및 모델 관련 정보들을 가지고 있는 딕셔너리\n",
    "    '''\n",
    "    print(f'Model Type is XGBoost')\n",
    "    \n",
    "    # 학습에 사용할 설명변수 명 지정\n",
    "    xvar_name = TRAIN_ANAL_DAT['x_var']\n",
    "    \n",
    "    # XGBoost 학습 데이터 준비\n",
    "    train_sparse_mat = xgb.DMatrix(data = TRAIN_ANAL_DAT['train_dat'][xvar_name], label = TRAIN_ANAL_DAT['train_dat'][yvar_name])\n",
    "    \n",
    "    # Grid Search\n",
    "    params = {'max_depth':[5,7],\n",
    "              'min_child_weight':[1.0,3.0],\n",
    "              'colsample_bytree':[0.5,0.75]}\n",
    "    params_grid = pd.DataFrame(ParameterGrid(params))\n",
    "    \n",
    "    score_list           = []\n",
    "    num_boost_round_list = []\n",
    "    for params_idx, params in params_grid.iterrows() :\n",
    "        params_tmp  = {'max_depth'       : int(params['max_depth']),\n",
    "                       'min_child_weight': float(params['min_child_weight']),\n",
    "                       'colsample_bytree': float(params['colsample_bytree'])}\n",
    "        xgb_cv      = xgb.cv(dtrain = train_sparse_mat, params = params_tmp, num_boost_round = 200, nfold = 3, early_stopping_rounds = 10, maximize = 0, verbose_eval= 0, seed =1234)\n",
    "        num_boost_round_list.append(xgb_cv.shape[0])\n",
    "        score_list.append(xgb_cv['test-rmse-mean'].iloc[-1])\n",
    "    \n",
    "    # Find Best Parameter\n",
    "    params_grid['num_boost_round'] = num_boost_round_list\n",
    "    params_grid['score']           = score_list\n",
    "    best_params = params_grid.iloc[np.argmin(params_grid['score']),:]\n",
    "    xgb_train_params = {'max_depth'       : int(best_params['max_depth']),\n",
    "                        'min_child_weight': float(best_params['min_child_weight']),\n",
    "                        'colsample_bytree': float(best_params['colsample_bytree'])}\n",
    "    num_boost_round = int(best_params['num_boost_round'])    \n",
    "    \n",
    "    # XGBoost Fitting\n",
    "    xgb_model = xgb.train(dtrain = train_sparse_mat, params = xgb_train_params, num_boost_round = num_boost_round)\n",
    "    \n",
    "    # 학습/검증데이터 예측\n",
    "    TRAIN_ANAL_DAT['train_dat']['pred'] = xgb_model.predict(xgb.DMatrix(TRAIN_ANAL_DAT['train_dat'][TRAIN_ANAL_DAT['x_var']]))\n",
    "    TRAIN_ANAL_DAT['test_dat']['pred']  = xgb_model.predict(xgb.DMatrix(TRAIN_ANAL_DAT['test_dat'][TRAIN_ANAL_DAT['x_var']]))\n",
    "    \n",
    "    ret = dict({'model' : xgb_model,'model_name' : 'XGB' , \"yvar\" : yvar_name, \"xvar\" : xvar_name, \"train_res\" : TRAIN_ANAL_DAT['train_dat'], \"test_res\" : TRAIN_ANAL_DAT['test_dat']})\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type is XGBoost\n"
     ]
    }
   ],
   "source": [
    "XGB_RESULT = MODELING_XGB(yvar_name = 'y_암모니아', dat = TRAIN_ANAL_DAT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
